\documentclass{beamer}
\usepackage{amsmath}
\usetheme[numbering=fraction]{metropolis}
\usepackage[T1]{fontenc}
\usepackage[default]{lato}
\usepackage{mathpazo}
\usepackage{textcomp}
\usepackage{mathtools}
\usefonttheme{professionalfonts}
\usefonttheme[onlymath]{serif}

\overfullrule=2cm

\setbeamertemplate{footline}{%
  \begin{beamercolorbox}[wd=\textwidth, sep=1ex]{footline}%
    \usebeamerfont{page number in head/foot}%
    \usebeamertemplate*{frame footer}
    \hfill%
    \usebeamertemplate*{frame numbering}
    \quad% 
  \end{beamercolorbox}%
}

\usepackage[absolute,overlay]{textpos}
% \usepackage[texcoord, grid,gridcolor=red!10,subgridcolor=green!10,gridunit=pt] {eso-pic}

%\usepackage[giveninits=true, style=verbose]{biblatex}
%\addbibresource{presentation.bib}

\usepackage{siunitx}

% Table packages.
\usepackage{booktabs}

\usepackage{makecell}
\usepackage{tikz}

\usepackage[beamer,customcolors]{hf-tikz}
\usetikzlibrary{calc}

% This block allows to include images and bib files relative to this TeX file.
%\newcommand{\PathToTexFile}{02-presentation-v1}
\graphicspath{assets/}

\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\texttt#1{<#1>}%
}

\setbeamercolor{background canvas}{fg=black,bg=white}

% Define useful colors.
\colorlet{black}{black!50!gray}
\colorlet{green}{green!50!gray}
\colorlet{blue}{blue!50!gray}
%\colorlet{red}{red!5}

\colorlet{CBlue}{blue!15}
\colorlet{CBlueD}{blue!50!gray}
\colorlet{CRed}{red!15}
\colorlet{CRedD}{red!50!gray}
\colorlet{CGreenD}{green!60!gray}
\colorlet{COrange}{orange}

% tikz settings
\tikzset{   
    every path/.style={thick},
}

\usepackage[protrusion,expansion,kerning,spacing,final]{microtype}
\microtypecontext{spacing=nonfrench}

\title{Estimating divergence-free flows via neural networks}
\author{%
    D.~I.~Kabanov \inst{1} \and %
    L.~Espath \inst{1} \and %
    J.~Kiessling \inst{2} \and %
    R.~F.~Tempone \inst{1,3}
}
\institute{
    \inst{1} RWTH Aachen, Germany \and %
    \inst{2} KTH, Sweden \and %
    \inst{3} KAUST, Saudi Arabia
}
\date{18 March 2021, GAMM 91st Annual Meeting}
\def\titlepage{%
  \usebeamertemplate{title page}%<---
}
%\titlegraphic{\vspace{6cm}\hspace{4.3cm}\includegraphics{rwth-logo}\phantom{text}\par}

% ------------------------------------------------------------------------------
% Useful mathematical macros
\newcommand{\Data}{\vec{D}}
\newcommand{\DataExt}{\widetilde{\vec{D}}}
\newcommand{\MSE}{\ensuremath{\text{MSE}}}
\newcommand{\T}{\ensuremath{\text{T}}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\VTheta}{\ensuremath{\vec{\theta}}}
\newcommand{\VLambda}{\ensuremath{\vec{\lambda}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\R}{\mathbb R}
\newcommand{\UNN}[1][\text{NN}]{u_{#1}}
\newcommand{\FNN}[1][\text{NN}]{f_{#1}}
\newcommand{\NonlinOp}{\mathcal N\!}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

% Modify margins for a slide.
% https://tex.stackexchange.com/questions/160825/modifying-margins-for-one-slide
\newcommand\Wider[2][3em]{%
\makebox[\linewidth][c]{%
  \begin{minipage}{\dimexpr\textwidth+#1\relax}
  \raggedright#2
  \end{minipage}%
  }%
}
% Useful mathematical macros (end)
% ------------------------------------------------------------------------------

\begin{document}
\maketitle

% ------------------------------------------------------------------------------
% Common part
\begin{frame}{Outline}
\begin{itemize}
    \item Problem formulation
    \item How neural networks are used for the problem
    \item Example 1: synthetic dataset
    \item Example 2: real wind dataset from Sweden
\end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}{Problem formulation}

Consider dataset
\[
    \vec{D} = \left\{\vec{x}_i, \vec{u}_i\right\}, \quad i = 1, ..., N
\]
where $\vec{x}_i = (x_i, y_i) \in \mathcal D \subset \R^2$ is a spatial point,
$\vec{u}_i = (u_i, v_i) \in \R^2$ is the velocity field at point $i$.

Assume that the velocity field $\vec u (\vec x)$ satisfies the
divergence-free condition, at least approximately:
\[
    \nabla \cdot \vec u (\vec x) \approx 0.
\]

\textbf{Goal}: estimate the velocity field $\vec u  (\vec x)$ using the above
condition.
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}{Problem formulation}
Instead of the true velocity field, we seek for estimator
\[
    \hat{\vec u} (\vec x; \vec \theta)
\]
parameterized by
\[
    \vec \theta \in \R^n
\]    
that satisfies
the measurements and the divergence-free condition
\[
    \arg \min_{\vec \theta} \quad 
        \mathbb E \left[ \norm{\vec u(\vec x) - \hat{\vec u}(\vec x; \vec{\theta})}^2_{L^2(\R^2)} \right] +
        \gamma \norm{\nabla \cdot \hat{\vec u}(\vec x; \vec{\theta})}^2_{L^2(\R)} 
\]
where $\gamma$ is a regularization parameter that determines how strongly
the divergence-free condition is enforced.
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}{Problem formulation}
\[
    \arg \min_\theta \quad 
        \mathbb E \left[ \norm{\vec u(\vec x) - \hat{\vec u}(\vec x; \vec{\theta})}^2_{L^2(\R^2)} \right] +
        \gamma \norm{\nabla \cdot \hat{\vec u}(\vec x; \vec{\theta})}^2_{L^2(\R)} 
\]

Finite number of measurements to evaluate the first term. 

Introduce an additional set of $P$ points, at which
the divergence-free condition is enforced.

Then the problem is to find
\[
    \arg \min_\theta \quad
        \frac{1}{N} \sum_{i=1}^N \norm{\vec u_i - \hat{\vec u}(\vec x_i; \theta)}^2_2
        \ + \;
        \frac{\gamma}{P} \sum_{i=1}^P \left( \nabla \cdot \hat{\vec u}(\vec x_i; \theta)\right)^2
\]
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{Estimator function}

We use a neural network as an estimator function $\hat{\vec u}(\vec x; \theta)$.

This neural network is of type ''multi-layer perceptron``:
\[
    \hat{\vec u}(\vec x; \vec \theta) = A \circ F_{L-1} \circ \dots \circ F_1(\vec x)
\]
where
\begin{itemize}
 \item Hidden layers $F_j : \R^{n_{j-1}} \to \R^{n_j}$ are defined as
\[
    F_j\left( \vec z_{j-1} \right) = \sigma \left( \mat W_j \vec z_{j-1} + \vec b_j \right)
\]
with $\sigma$ being a nonlinear function applied componentwise

\item $A(z_L) = W_L z_L + b_L$ the output layer

\item Parameter vector $\vec \theta$ is a concatenation of weight matrices $\mat W_j$
and bias vectors $\vec b_j$, $j=1,\dots,L$.
\end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{Visualization of a neural network $\R^2 \to \R^2$}

\begin{figure}[t]
    \centering
    \def\layersep{2.5cm}

    \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
        \tikzstyle{every pin edge}=[<-,shorten <=1pt]
        \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
        \tikzstyle{input neuron}=[neuron, fill=CGreenD];
        \tikzstyle{output neuron}=[neuron, fill=CRedD];
        \tikzstyle{hidden neuron}=[neuron, fill=CBlueD];
        \tikzstyle{annot} = [text width=4em, text centered]
    
        % Draw the input layer nodes
        \foreach \name / \y in {1,...,2}
        % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
            \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-1-\y) {};
    
        % Draw the hidden layer nodes
        \foreach \name / \y in {1,...,5}
            \path[yshift=0.5cm]
                node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
    
        % Draw the output layer node
        \node[output neuron,pin={[pin edge={->}]right:Output \#1}, right of=H-2] (O1) at (2.5, -2) {};
        \node[output neuron,pin={[pin edge={->}]right:Output \#2}, right of=H-4] (O2) at (2.5, -3) {};
    
        % Connect every node in the input layer with every node in the
        % hidden layer.
        \foreach \source in {1,...,2}
            \foreach \dest in {1,...,5}
                \path (I-\source) edge (H-\dest);
    
        % Connect every node in the hidden layer with the output layer
        \foreach \source in {1,...,5}
            \path (H-\source) edge (O1);

        \foreach \source in {1,...,5}
            \path (H-\source) edge (O2);
    
        % Annotate the layers
        \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
        \node[annot,left of=hl] {Input layer};
        \node[annot,right of=hl] {Output layer};
    \end{tikzpicture}
    %\caption[Network graph for a $(L+1)$-layer perceptron.]{Network graph of a $(L+1)$-layer perceptron with $D$ input units and $C$ output units. The $l^{\text{th}}$ hidden layer contains $m^{(l)}$ hidden units.}
    \label{fig:multilayer-perceptron}
\end{figure}

\begin{minipage}{\linewidth}
\centering
\tiny[\url{https://texample.net/tikz/examples/neural-network/}, with modifications]
\end{minipage}
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{Other details}

\begin{itemize}
    \item Points where divergence-free condition is enforced, are on the uniform grid
    \item To solve the optimization problem, we use ADAM optimizer
    \item Exponential averaging of the parameters is used for prediction
    \item Code is written using libraries Keras and Tensorflow 2
\end{itemize}

\end{frame}

\section{Example 1}

% -----------------------------------------------------------------------------
% Example 1.
\begin{frame}
\frametitle{Example 1}

Dataset of synthetic noiseless data
\[
    \left\{\vec{x}_i, \vec{u}_i\right\}, \quad i = 1, ..., 10
\]
where
\[
    \vec u(x, y) = \begin{pmatrix}
        \phantom{-} \cos x \, \sin y\\
        -\sin x \, \cos y
    \end{pmatrix}
\]
and points $\vec x_i = (x_i, y_i)$ are sampled uniformly from domain $[0; 2\pi]^2$.

This dataset is a Taylor--Green 2D vortex fixed in time.

Test data are $21\times21$ uniform grid in this domain.
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{Example 1, true field}

\vspace{1cm}
\begin{figure}
\centering
\includegraphics[scale=0.65]{assets/stream-plot-true.pdf}
\end{figure}
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{Model performance score}

Model performance is measured with $R^2$ score:    
\[
    R^2 = 1 - \frac{
        \sum_{i=1}^N \norm{\vec u_i - \hat{\vec u}(\vec x; \theta)}^2_2
    }{
        \sum_{i=1}^N \norm{\vec u_i}^2_2
    }
\]

\vspace{0.5cm}
Interpretation:
\begin{itemize}
    \item measure model performance relative to the model that
          predicts zero velocity field at each point
    \item perfect model has $R^2=1$.
\end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{$R^2$ score versus $\gamma$}
Training by minimizing misfit + $\gamma$ div-free condition

\vspace{0.5cm}
\begin{figure}
\centering
\includegraphics[scale=0.65]{assets/R2-vs-gamma.pdf}    
\end{figure}
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame}
\frametitle{Error fields}
Training by minimizing misfit + $\gamma$ div-free condition

\begin{textblock*}{50pt}(90pt,90pt)
    $\gamma = 0$
\end{textblock*}
\begin{textblock*}{50pt}(240pt,90pt)
    $\gamma = 10^{-2}$
\end{textblock*}

\vspace{0.5cm}
\Wider[8em]{%
\begin{figure}
\centering
\includegraphics[scale=0.57]{assets/{error-fields-comparison}.pdf}
\end{figure}
}
\end{frame}

% -----------------------------------------------------------------------------


\begin{frame}[standout]
\Huge
Thank you!
\end{frame}

\end{document}